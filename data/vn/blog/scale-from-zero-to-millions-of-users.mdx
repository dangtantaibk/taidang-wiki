---
title: 'Quy mô từ zero đến hàng triệu người dùng'
date: '2023-11-23'
tags: ['system-design']
draft: false
summary: 'Thiết kế một hệ thống có thể hỗ trợ hàng triệu người dùng là một thách thức lớn, 
và nó là một hành trình đòi hỏi sự sàng lọc và cải tiến không ngừng.
Trong bài viết này, chúng ra sẽ xây dựng một hệ thống có thể hỗ trợ từ một người dùng và 
dần dần mở rộng nó cho hàng triệu người dùng.'
images: ['/static/images/millions-users.jpeg']
authors: ['default']
---

import Twemoji from './Twemoji.tsx'
import UnsplashPhotoInfo from './UnsplashPhotoInfo.tsx'

![thumbnail-image](/static/images/millions-users.jpeg)

## Thiết lập Single server
> Một hành trình hàng ngàn dặm bắt đầu bằng một bước đi, và xây dựng một hệ thống phức tập cũng không ngoại lệ.

Để bắt đầu với một thứ đơn giản, mọi thứ đều chạy trên một single server. Hình 1-1 cho thấy hình minh hoạ
của việc cấu hình trên 1 single server, nơi mọi người thì được chạy trên một máy chủ: *web-app*, *database*, *cache*, etc.

![figure1-1](/static/images/System-Design-Interview/figure1-1.jpg)

Dể hiểu các cấu hình này, thì việc tìm hiểu về request flow và traffic source sẽ rất hữu ích. Bắt đầu với request flow bên dưới (Hình 1-2)

![figure1-2](/static/images/System-Design-Interview/figure1-2.jpg)

1. Người dùng truy cập trang web dựa vào tên domain, giống như api.mysite.com. Thông thường, **Domain Name System (DNS)**
là một dịch vụ trả phí được cung cấp bởi bên thứ 3 và không được máy chủ của chúng ta lưu trữ.
2. Địa chỉ **Internet Protocol (IP)** sẽ được trả về cho browser hoặc ứng dụng Mobile. Trong ví dụ, IP address 15.255.23.214
thì được trả về.
3. Sau khi có được địa chỉ IP, yêu cầu Hypertext Transfer Protocol (HTTP) [1] được gửi thẳng tới máy chủ của bạn.

4. Web server sẽ trả về trang HTML có response dạng JSON để hiển thị.

Tiếp tục, chúng ta hãy kiểm tra traffic source. Lưu lượng truy cập vào máy chủ web của bạn được đến từ 2 nguồn: Ứng dụng web và ứng dụng mobile.

 - Ứng dụng Web: nó sử dụng kết hợp với các ngôn ngữ phía máy chủ (Java, Python, etc.) để xử lý các logic nghiệp vụ, lưu trữ, etc., và
 ngôn ngữ máy khách (HTML and JavaScript) để hiển thị.
 - Ứng dụng Mobile: **Phương thức HTTP** là phương thức trao đổi giữa ứng dụng mobile và máy chủ web. **JavaScript
 Object Notation (JSON)** là định dạng response API thường được sử dụng để truyền dữ liệu do tính đơn giản của nó. 
 Một ví dụ của API response trong định dạng JSON được hiển thị bên dưới:

`GET /user/12 - Retrieve user object for id = 12`
``` JSON
{
    "id": 12,
    "firstName": "Tài",
    "lastName": "Đặng",
    "address": {
        "streetAddress": "Chung cư Sinh lợi",
        "city": "Hồ Chí Minh",
        "state": "HCM",
        "postalCode": 70000
    },
    "phoneNumbers": [
        "0978507xxx",
        "0868146xxx"
    ]
}
``` 
## Database
Với sự phát triển của cơ sở người dùng, một máy chủ là không đủ, và chúng ta cần nhiều máy chủ: một cho web/mobile traffic,
một cho database (Hình 1-3). Việc tách web/mobile traffice (Tầng web) và database (tầng dữ liệu) cho phép chúng ta có thể 
mở rộng quy mô một cách độc lập.

![figure1-3](/static/images/System-Design-Interview/figure1-3.jpg)

## Database nào được sử dụng?
Bạn có thể chọn giữa cơ sở dữ liệu quan hệ truyền thống và cơ sở dữ liệu phi quan hệ. Chúng ta hãy xem xét sự khác biệt của họ.

Cơ sở dữ liệu quan hệ cũng được gọi là **relational database management system (RDBMS)** hoặc **SQL database**.
Các database phổ biến có thể bao gồm: MySQL, Oracle, PostgreSQL, etc. Cơ sở dữ liệu quan hệ biểu diễn và lưu trữ 
dữ liệu trong table và row. Bạn có thể thực hiện phép join bằng SQL trên các table khác nhau.

Cơ sở dữ liệu phi quan hệ cũng được gọi là **NoSQL database**. Các database phổ biến bao gồm: CouchDB, Neo4j, Cassandra, HBase, Amazon DynamoDB, etc. [2]
Những database này thường được chia làm 4 loại: key-value stores, graph stores, column stores, document stores.
Phép Join thường sẽ không được hỗ trợ trên các cơ sở dữ liệu phi quan hệ.

Cho hầu hết nhà phát triển, cơ sở dữ liệu quan hệ là một lựa chọn tốt nhất bởi vì nó đã tồn tại trong 40 năm và lịch sử cho thấy, 
nó vẫn làm việc tốt. Tuy nhiên, nếu cơ sở dữ liệu quan hệ không đáp ứng cho trường hợp cụ thể của bạn, bạn có thể cân nhắc
các lựa chọn không phải cơ sở dữ liệu quan hệ. Cơ sở dữ liệu phi quan hệ được chọn nếu: 
- Ứng dụng của bạn yêu cầu độ trễ siêu thấp.
- Dữ liệu của bạn thì không có cấu trúc, hoặc bạn không có bất kỳ quan hệ của dữ liệu.
- Bạn chỉ cần serialize and deserialize dữ liệu (JSON, XML, YAML, etc.).
- Bạn cần lưu trữ một lượng lớn dữ liệu.

## Mở rộng theo chiều dọc (Vertical scaling) vs Mở rộng theo chiều ngang (Horizontal scaling)
Vertical scaling, gọi là *“scale up”*, có nghĩa là bổ sung thêm sức mạng (CPU, RAM, etc.) cho máy chủ của bạn. 
Horizontal scaling, gọi là *“scale-out”*, cho phép bạn mở rộng bằng cách thêm nhiều máy chủ vào nhóm tài nguyên của mình.

Khi traffic còn thấp, vertical scaling là một lựa chọn tốt, và sự đơn giản của vertical scaling là ưu điểm chính của nó.

Không may, nó đi kèm với các hạn chế nghiêm trọng.
- Vertical scaling có một giới hạn cứng. Không thể thêm không giới hạn CPU và memoru vào một máy chủ.
- Vertical scaling không có failover và dự phòng. Nếu một máy chủ down, website và ứng dụng của bạn cũng down ngay lập tức.

Horizontal scaling is more desirable for large scale applications due to the limitations of vertical scaling.

In the previous design, users are connected to the web server directly. Users will unable to access the website if the web server is offline. 
In another scenario, if many users access the web server simultaneously and it reaches the web server’s load limit, 
users generally experience slower response or fail to connect to the server. A load balancer is the best technique to 
address these problems.

## Load balancer
A load balancer evenly distributes incoming traffic among web servers that are defined in a load-balanced set. 
Figure 1-4 shows how a load balancer works.

![figure1-4](/static/images/System-Design-Interview/figure1-4.jpg)

As shown in Figure 1-4, users connect to the public IP of the load balancer directly. With this setup, web servers 
are unreachable directly by clients anymore. For better security, private IPs are used for communication between 
servers. A private IP is an IP address reachable only between servers in the same network; however, it is unreachable 
over the internet. The load balancer communicates with web servers through private IPs.

In Figure 1-4, after a load balancer and a second web server are added, we successfully solved no failover issue and 
improved the availability of the web tier. Details are explained below:
- If server 1 goes offline, all the traffic will be routed to server 2. This prevents the website from going offline.
We will also add a new healthy web server to the server pool to balance the load.
- If the website traffic grows rapidly, and two servers are not enough to handle the traffic, the load balancer can handle 
this problem gracefully. You only need to add more servers to the web server pool, and the load balancer automatically 
starts to send requests to them.

Now the web tier looks good, what about the data tier? The current design has one database, so it does not support failover 
and redundancy. Database replication is a common technique to address those problems. Let us take a look.

## Database replication

> Quoted from Wikipedia: “Database replication can be used in many database management systems, 
usually with a master/slave relationship between the original (master) and the copies (slaves)” [3].

A master database generally only supports write operations. A slave database gets copies of the data from the master 
database and only supports read operations. All the data-modifying commands like insert, delete, or update must be 
sent to the master database. Most applications require a much higher ratio of reads to writes; thus, the number of 
slave databases in a system is usually larger than the number of master databases. Figure 1-5 shows a master database
with multiple slave databases.

![figure1-5](/static/images/System-Design-Interview/figure1-5.jpg)

Advantages of database replication:
- **Better performance:** In the master-slave model, all writes and updates happen in master nodes; whereas, 
read operations are distributed across slave nodes. This model improves performance because it allows more 
queries to be processed in parallel.
- **Reliability:** If one of your database servers is destroyed by a natural disaster, such as a typhoon or 
an earthquake, data is still preserved. You do not need to worry about data loss because data is replicated
 across multiple locations.
- **High availability:** By replicating data across different locations, your website remains in operation even
 if a database is offline as you can access data stored in another database server.

In the previous section, we discussed how a load balancer helped to improve system availability. 
We ask the same question here: what if one of the databases goes offline? The architectural design
 discussed in Figure 1-5 can handle this case:
- If only one slave database is available and it goes offline, read operations will be directed to the 
master database temporarily. As soon as the issue is found, a new slave database will replace the old one. 
In case multiple slave databases are available, read operations are redirected to other healthy slave 
databases. A new database server will replace the old one.
- If the master database goes offline, a slave database will be promoted to be the new master. All the 
database operations will be temporarily executed on the new master database. A new slave database will 
replace the old one for data replication immediately. In production systems, promoting a new master is
 more complicated as the data in a slave database might not be up to date. The missing data needs to be 
 updated by running data recovery scripts. Although some other replication methods like multi-masters and 
 circular replication could help, those setups are more complicated; and their discussions are beyond the 
 scope of this book. Interested readers should refer to the listed reference materials [4] [5].

Figure 1-6 shows the system design after adding the load balancer and database replication.

![figure1-6](/static/images/System-Design-Interview/figure1-6.jpg)

Let us take a look at the design:
- A user gets the IP address of the load balancer from DNS.
- A user connects the load balancer with this IP address.
- The HTTP request is routed to either Server 1 or Server 2.
- A web server reads user data from a slave database.
- A web server routes any data-modifying operations to the master database. This includes write, 
update, and delete operations.

Now, you have a solid understanding of the web and data tiers, it is time to improve the load/response time. 
This can be done by adding a cache layer and shifting static content (JavaScript/CSS/image/video files) 
to the **content delivery network (CDN)**.

## Cache
A cache is a temporary storage area that stores the result of expensive responses or frequently accessed 
data in memory so that subsequent requests are served more quickly. As illustrated in Figure 1-6, every 
time a new web page loads, one or more database calls are executed to fetch data. The application performance 
is greatly affected by calling the database repeatedly. The cache can mitigate this problem.

## Cache tier
The cache tier is a temporary data store layer, much faster than the database. The benefits of 
having a separate cache tier include better system performance, ability to reduce database workloads, 
and the ability to scale the cache tier independently. Figure 1-7 shows a possible setup of a cache server:

![figure1-7](/static/images/System-Design-Interview/figure1-7.jpg)

After receiving a request, a web server first checks if the cache has the available response. 
If it has, it sends data back to the client. If not, it queries the database, stores the response in cache, 
and sends it back to the client. This caching strategy is called a read-through cache. Other caching 
strategies are available depending on the data type, size, and access patterns. A previous study 
explains how different caching strategies work [6].

Interacting with cache servers is simple because most cache servers provide APIs for common programming 
languages. 
The following code snippet shows typical Memcached APIs:

``` Java
int SECONDS = 1;
cache.set('myKey', 'hi threr'. 3600 * SECONDS);
cache.get('myKey');
```

## Considerations for using cache
Here are a few considerations for using a cache system:
- Decide when to use cache. Consider using cache when data is read frequently but modified infrequently. 
Since cached data is stored in volatile memory, a cache server is not ideal for persisting data. For 
instance, if a cache server restarts, all the data in memory is lost. Thus, important data should be 
saved in persistent data stores.
- Expiration policy. It is a good practice to implement an expiration policy. Once cached data is 
expired, it is removed from the cache. When there is no expiration policy, cached data will be stored 
in the memory permanently. It is advisable not to make the expiration date too short as this will cause 
the system to reload data from the database too frequently. Meanwhile, it is advisable not to make the 
expiration date too long as the data can become stale.
- Consistency: This involves keeping the data store and the cache in sync. Inconsistency can happen 
because data-modifying operations on the data store and cache are not in a single transaction. When 
scaling across multiple regions, maintaining consistency between the data store and cache is 
challenging. For further details, refer to the paper titled “Scaling Memcache at Facebook” 
published by Facebook [7].
- Mitigating failures: A single cache server represents a potential single point of failure (SPOF),
 defined in Wikipedia as follows: “A single point of failure (SPOF) is a part of a system that, 
 if it fails, will stop the entire system from working” [8]. As a result, multiple cache servers 
 across different data centers are recommended to avoid SPOF. Another recommended approach is to 
 overprovision the required memory by certain percentages. This provides a buffer as the memory 
 usage increases.

![figure1-8](/static/images/System-Design-Interview/figure1-8.jpg)

- Eviction Policy: Once the cache is full, any requests to add items to the cache might cause 
existing items to be removed. This is called cache eviction. Least-recently-used (LRU) is the 
most popular cache eviction policy. Other eviction policies, such as the Least Frequently Used (LFU) 
or First in First Out (FIFO), can be adopted to satisfy different use cases.

## Content delivery network (CDN)
A CDN is a network of geographically dispersed servers used to deliver static content. CDN servers
cache static content like images, videos, CSS, JavaScript files, etc.

Dynamic content caching is a relatively new concept and beyond the scope of this book. It enables
the caching of HTML pages that are based on request path, query strings, cookies, and request 
headers. Refer to the article mentioned in reference material [9] for more about this. 
This book focuses on how to use CDN to cache static content.

Here is how CDN works at the high-level: when a user visits a website, a CDN server closest to 
the user will deliver static content. Intuitively, the further users are from CDN servers, the 
slower the website loads. For example, if CDN servers are in San Francisco, users in Los Angeles 
will get content faster than users in Europe. Figure 1-9 is a great example that shows how CDN 
improves load time.

![figure1-9](/static/images/System-Design-Interview/figure1-9.jpg)

![figure1-10](/static/images/System-Design-Interview/figure1-10.jpg)

1. User A tries to get image.png by using an image URL. The URL’s domain is provided by the CDN 
provider. The following two image URLs are samples used to demonstrate what image URLs look like 
on Amazon and Akamai CDNs:
    - https://mysite.cloudfront.net/logo.jpg
    - https://mysite.akamai.com/image-manager/img/logo.jpg
2. If the CDN server does not have image.png in the cache, the CDN server requests the file from 
the origin, which can be a web server or online storage like Amazon S3.
3. The origin returns image.png to the CDN server, which includes optional HTTP header 
Time-to-Live (TTL) which describes how long the image is cached.
4. The CDN caches the image and returns it to User A. The image remains cached in the CDN 
until the TTL expires.
5. User B sends a request to get the same image.
6. The image is returned from the cache as long as the TTL has not expired.

## Considerations of using a CDN
- Cost: CDNs are run by third-party providers, and you are charged for data transfers in and 
out of the CDN. Caching infrequently used assets provides no significant benefits so you should 
consider moving them out of the CDN.
- Setting an appropriate cache expiry: For time-sensitive content, setting a cache expiry time 
is important. The cache expiry time should neither be too long nor too short. If it is too long, 
the content might no longer be fresh. If it is too short, it can cause repeat reloading of content 
from origin servers to the CDN.
- CDN fallback: You should consider how your website/application copes with CDN failure. If there 
is a temporary CDN outage, clients should be able to detect the problem and request resources 
from the origin.
- Invalidating files: You can remove a file from the CDN before it expires by performing one of 
the following operations:
    - Invalidate the CDN object using APIs provided by CDN vendors.
    - Use object versioning to serve a different version of the object. To version an object, 
    you can add a parameter to the URL, such as a version number. For example, version number 2 
    is added to the query string: image.png?v=2.

Figure 1-11 shows the design after the CDN and cache are added.

![figure1-11](/static/images/System-Design-Interview/figure1-11.jpg)

1. Static assets (JS, CSS, images, etc.,) are no longer served by web servers. They are fetched 
from the CDN for better performance.
2. The database load is lightened by caching data.

## Stateless web tier
Now it is time to consider scaling the web tier horizontally. For this, we need to move state 
(for instance user session data) out of the web tier. A good practice is to store session data 
in the persistent storage such as relational database or NoSQL. Each web server in the cluster 
can access state data from databases. This is called stateless web tier.

## Stateful architecture
A stateful server and stateless server has some key differences. A stateful server remembers 
client data (state) from one request to the next. A stateless server keeps no state information.
Figure 1-12 shows an example of a stateful architecture.

![figure1-12](/static/images/System-Design-Interview/figure1-12.jpg)

In Figure 1-12, user A’s session data and profile image are stored in Server 1. To authenticate 
User A, HTTP requests must be routed to Server 1. If a request is sent to other servers like 
Server 2, authentication would fail because Server 2 does not contain User A’s session data. 
Similarly, all HTTP requests from User B must be routed to Server 2; all requests from User 
C must be sent to Server 3.

The issue is that every request from the same client must be routed to the same server. This 
can be done with sticky sessions in most load balancers [10]; however, this adds the overhead. 
Adding or removing servers is much more difficult with this approach. It is also challenging 
to handle server failures.

## Stateless architecture

Figure 1-13 shows the stateless architecture.

![figure1-13](/static/images/System-Design-Interview/figure1-13.jpg)

In this stateless architecture, HTTP requests from users can be sent to any web servers, 
which fetch state data from a shared data store. State data is stored in a shared data store 
and kept out of web servers. A stateless system is simpler, more robust, and scalable.

Figure 1-14 shows the updated design with a stateless web tier.

![figure1-14](/static/images/System-Design-Interview/figure1-14.jpg)

In Figure 1-14, we move the session data out of the web tier and store them in the 
persistent data store. The shared data store could be a relational database, Memcached/Redis, 
NoSQL, etc. The NoSQL data store is chosen as it is easy to scale. Autoscaling means adding 
or removing web servers automatically based on the traffic load. After the state data is 
removed out of web servers, auto-scaling of the web tier is easily achieved by adding or 
removing servers based on traffic load.

Your website grows rapidly and attracts a significant number of users internationally. To 
improve availability and provide a better user experience across wider geographical areas, 
supporting multiple data centers is crucial.

## Data centers


Figure 1-15 shows an example setup with two data centers. In normal operation, users are 
geoDNS-routed, also known as geo-routed, to the closest data center, with a split traffic 
of x% in US-East and (100 – x)% in US-West. geoDNS is a DNS service that allows domain 
names to be resolved to IP addresses based on the location of a user.

![figure1-15](/static/images/System-Design-Interview/figure1-15.jpg)

In the event of any significant data center outage, we direct all traffic to a healthy 
data center. In Figure 1-16, data center 2 (US-West) is offline, and 100% of the traffic 
is routed to data center 1 (US-East).

![figure1-16](/static/images/System-Design-Interview/figure1-16.jpg)

Several technical challenges must be resolved to achieve multi-data center setup:
- Traffic redirection: Effective tools are needed to direct traffic to the correct data 
center. GeoDNS can be used to direct traffic to the nearest data center depending on 
where a user is located.
- Data synchronization: Users from different regions could use different local databases 
or caches. In failover cases, traffic might be routed to a data center where data is 
unavailable. A common strategy is to replicate data across multiple data centers. 
A previous study shows how Netflix implements asynchronous multi-data center replication [11].
- Test and deployment: With multi-data center setup, it is important to test your 
website/application at different locations. Automated deployment tools are vital 
to keep services consistent through all the data centers [11].

To further scale our system, we need to decouple different components of the system 
so they can be scaled independently. Messaging queue is a key strategy employed by 
many real- world distributed systems to solve this problem.

## Message queue

A message queue is a durable component, stored in memory, that supports asynchronous 
communication. It serves as a buffer and distributes asynchronous requests. The basic 
architecture of a message queue is simple. Input services, called producers/publishers, 
create messages, and publish them to a message queue. Other services or servers, called 
consumers/subscribers, connect to the queue, and perform actions defined by the messages. 
The model is shown in Figure 1-17.

![figure1-17](/static/images/System-Design-Interview/figure1-17.jpg)

Decoupling makes the message queue a preferred architecture for building a scalable and 
reliable application. With the message queue, the producer can post a message to the 
queue when the consumer is unavailable to process it. The consumer can read messages 
from the queue even when the producer is unavailable.

Consider the following use case: your application supports photo customization, including 
cropping, sharpening, blurring, etc. Those customization tasks take time to complete. 
In Figure 1-18, web servers publish photo processing jobs to the message queue. Photo 
processing workers pick up jobs from the message queue and asynchronously perform photo 
customization tasks. The producer and the consumer can be scaled independently. When the 
size of the queue becomes large, more workers are added to reduce the processing time. 
However, if the queue is empty most of the time, the number of workers can be reduced.

![figure1-18](/static/images/System-Design-Interview/figure1-18.jpg)

## Logging, metrics, automation
When working with a small website that runs on a few servers, logging, metrics, and 
automation support are good practices but not a necessity. However, now that your site 
has grown to serve a large business, investing in those tools is essential.

Logging: Monitoring error logs is important because it helps to identify errors and 
problems in the system. You can monitor error logs at per server level or use tools 
to aggregate them to a centralized service for easy search and viewing.

Metrics: Collecting different types of metrics help us to gain business insights and 
understand the health status of the system. Some of the following metrics are useful:
    - Host level metrics: CPU, Memory, disk I/O, etc.
    - Aggregated level metrics: for example, the performance of the entire database 
    tier, cache tier, etc.
    - Key business metrics: daily active users, retention, revenue, etc.

Automation: When a system gets big and complex, we need to build or leverage automation 
tools to improve productivity. Continuous integration is a good practice, in which each 
code check-in is verified through automation, allowing teams to detect problems early. 
Besides, automating your build, test, deploy process, etc. could improve developer 
productivity significantly.

## Adding message queues and different tools
Figure 1-19 shows the updated design. Due to the space constraint, only one data center 
is shown in the figure.
    1. The design includes a message queue, which helps to make the system more loosely 
    coupled and failure resilient.
    2. Logging, monitoring, metrics, and automation tools are included.

![figure1-19](/static/images/System-Design-Interview/figure1-19.jpg)

As the data grows every day, your database gets more overloaded. It is time to scale the data tier.

## Database scaling

There are two broad approaches for database scaling: vertical scaling and horizontal scaling.

## Vertical scaling
Vertical scaling, also known as scaling up, is the scaling by adding more power (CPU, RAM, DISK, etc.) 
to an existing machine. There are some powerful database servers. According to Amazon Relational 
Database Service (RDS) [12], you can get a database server with 24 TB of RAM. This kind of powerful 
database server could store and handle lots of data. For example, stackoverflow.com in 2013 had 
over 10 million monthly unique visitors, but it only had 1 master database [13]. However, 
vertical scaling comes with some serious drawbacks:
    - You can add more CPU, RAM, etc. to your database server, but there are hardware limits. 
    If you have a large user base, a single server is not enough.
    - Greater risk of single point of failures.
    - The overall cost of vertical scaling is high. Powerful servers are much more expensive.

## Horizontal scaling
Horizontal scaling, also known as sharding, is the practice of adding more servers. Figure 1- 20 
compares vertical scaling with horizontal scaling.

![figure1-20](/static/images/System-Design-Interview/figure1-20.jpg)

Sharding separates large databases into smaller, more easily managed parts called shards. 
Each shard shares the same schema, though the actual data on each shard is unique to the shard.

Figure 1-21 shows an example of sharded databases. User data is allocated to a database 
server based on user IDs. Anytime you access data, a hash function is used to find the 
corresponding shard. In our example, user_id % 4 is used as the hash function. If the result
equals to 0, shard 0 is used to store and fetch data. If the result equals to 1, shard 
1 is used. The same logic applies to other shards.

![figure1-21](/static/images/System-Design-Interview/figure1-21.jpg)

![figure1-22](/static/images/System-Design-Interview/figure1-22.jpg)

The most important factor to consider when implementing a sharding strategy is the choice 
of the sharding key. Sharding key (known as a partition key) consists of one or more columns 
that determine how data is distributed. As shown in Figure 1-22, “user_id” is the sharding key. 
A sharding key allows you to retrieve and modify data efficiently by routing database queries 
to the correct database. When choosing a sharding key, one of the most important
criteria is to choose a key that can evenly distributed data.

Sharding is a great technique to scale the database but it is far from a perfect solution. 
It introduces complexities and new challenges to the system:

**Resharding data:** Resharding data is needed when 1) a single shard could no longer hold 
more data due to rapid growth. 2) Certain shards might experience shard exhaustion faster 
than others due to uneven data distribution. When shard exhaustion happens, it requires 
updating the sharding function and moving data around. Consistent hashing, which will be 
discussed in Chapter 5, is a commonly used technique to solve this problem.

**Celebrity problem:** This is also called a hotspot key problem. Excessive access to a 
specific shard could cause server overload. Imagine data for Katy Perry, Justin Bieber, 
and Lady Gaga all end up on the same shard. For social applications, that shard will be 
overwhelmed with read operations. To solve this problem, we may need to allocate a shard 
for each celebrity. Each shard might even require further partition.

**Join and de-normalization:** Once a database has been sharded across multiple servers, 
it is hard to perform join operations across database shards. A common workaround is to 
de- normalize the database so that queries can be performed in a single table.

In Figure 1-23, we shard databases to support rapidly increasing data traffic. 
At the same time, some of the non-relational functionalities are moved to a NoSQL data 
store to reduce the database load. Here is an article that covers many use cases of NoSQL [14].

![figure1-22](/static/images/System-Design-Interview/figure1-23.jpg)

## Millions of users and beyond
Scaling a system is an iterative process. Iterating on what we have learned in this chapter 
could get us far. More fine-tuning and new strategies are needed to scale beyond millions
of users. For example, you might need to optimize your system and decouple the system 
to even smaller services. All the techniques learned in this chapter should provide a 
good foundation to tackle new challenges. To conclude this chapter, we provide a summary 
of how we scale our system to support millions of users:

    - Keep web tier stateless
    - Build redundancy at every tier
    - Cache data as much as you can
    - Support multiple data centers
    - Host static assets in CDN
    - Scale your data tier by sharding
    - Split tiers into individual services
    - Monitor your system and use automation tools

Congratulations on getting this far! Now give yourself a pat on the back. Good job!



## Reference materials
[1] Hypertext Transfer Protocol: https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol

[2] Should you go Beyond Relational Databases: https://blog.teamtreehouse.com/should-you-go-beyond-relational-databases

[3] Replication: https://en.wikipedia.org/wiki/Replication_(computing)

[4] Multi-master replication: https://en.wikipedia.org/wiki/Multi-master_replication

[5] NDB Cluster Replication: Multi-Master and Circular Replication: https://dev.mysql.com/doc/refman/5.7/en/mysql-cluster-replication-multi-master.html

[6] Caching Stretegies and How to Choose the Right One: https://codeahoy.com/2017/08/11/caching-strategies-and-how-to-choose-the-right-one/

[7] R. Nishtala, "Facebook, Scaling Memcache at," 10th USENIX Symposium on Networked Systems Design and Implementation (NSDI ’13). 

[8] Single point of failure: https://en.wikipedia.org/wiki/Single_point_of_failure

[9] Amazon CloudFront Dynamic Content Delivery: https://aws.amazon.com/cloudfront/dynamic-content/

[10] Configure Sticky Sessions for Your Classic Load Balancer: https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-sticky-sessions.html

[11] Active-Active for Multi-Regional Resiliency: https://netflixtechblog.com/active-active-for-multi-regional-resiliency-c47719f6685b

[12] Amazon EC2 High Memory Instances: https://aws.amazon.com/ec2/instance-types/high-memory/

[13] What it takes to run Stack Overflow: http://nickcraver.com/blog/2013/11/22/what-it-takes-to-run-stack-overflow

[14] What The Heck Are You Actually Using NoSQL For: http://highscalability.com/blog/2010/12/6/what-the-heck-are-you-actually-using-nosql-for.html

Happy reading <Twemoji emoji="clinking-beer-mugs" />
